{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the cell that begins with the line \"# CLICK ME\" to select it. The first character in that line indicates that what follows is a comment in Python, so it is ignored when executing the cell. The rest of the cell is, believe it or not, a complete system for creating and training a state-of-the-art model for recognizing cats versus dogs. So, let's train it now! To do so, just press Shift-Enter on your keyboard, or press the Play button on the toolbar. Then wait a few minutes while the following things happen:\n",
    "\n",
    "1. A dataset called the [Oxford-IIIT Pet Dataset](http://www.robots.ox.ac.uk/~vgg/data/pets/) that contains 7,349 images of cats and dogs from 37 different breeds will be downloaded from the fast.ai datasets collection to the GPU server you are using, and will then be extracted.\n",
    "2. A *pretrained model* that has already been trained on 1.3 million images, using a competition-winning model will be downloaded from the internet.\n",
    "3. The pretrained model will be *fine-tuned* using the latest advances in transfer learning, to create a model that is specially customized for recognizing dogs and cats.\n",
    "\n",
    "The first two steps only need to be run once on your GPU server. If you run the cell again, it will use the dataset and model that have already been downloaded, rather than downloading them again. Let's take a look at the contents of the cell, and the results (<<first_training>>):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"489pt\" height=\"134pt\"\n",
       " viewBox=\"0.00 0.00 489.18 134.36\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 130.36)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-130.36 485.18,-130.36 485.18,4 -4,4\"/>\n",
       "<!-- model -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>model</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"217.09,-79.36 141.09,-79.36 137.09,-75.36 137.09,-29.36 213.09,-29.36 217.09,-33.36 217.09,-79.36\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"213.09,-75.36 137.09,-75.36 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"213.09,-75.36 213.09,-29.36 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"213.09,-75.36 217.09,-79.36 \"/>\n",
       "<text text-anchor=\"middle\" x=\"177.09\" y=\"-50.66\" font-family=\"Times,serif\" font-size=\"14.00\">architecture</text>\n",
       "</g>\n",
       "<!-- predictions -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>predictions</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"340.14\" cy=\"-54.36\" rx=\"50.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"340.14\" y=\"-50.66\" font-family=\"Times,serif\" font-size=\"14.00\">predictions</text>\n",
       "</g>\n",
       "<!-- model&#45;&gt;predictions -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>model&#45;&gt;predictions</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M217.49,-54.36C236.29,-54.36 259.19,-54.36 280.02,-54.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"280.06,-57.86 290.06,-54.36 280.06,-50.86 280.06,-57.86\"/>\n",
       "</g>\n",
       "<!-- inputs -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>inputs</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"50.05\" cy=\"-74.36\" rx=\"32.49\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.05\" y=\"-70.66\" font-family=\"Times,serif\" font-size=\"14.00\">inputs</text>\n",
       "</g>\n",
       "<!-- inputs&#45;&gt;model -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>inputs&#45;&gt;model</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M81.64,-69.47C95.15,-67.31 111.38,-64.71 126.54,-62.28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"127.51,-65.67 136.83,-60.64 126.4,-58.76 127.51,-65.67\"/>\n",
       "</g>\n",
       "<!-- loss -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>loss</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"454.18\" cy=\"-83.36\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"454.18\" y=\"-79.66\" font-family=\"Times,serif\" font-size=\"14.00\">loss</text>\n",
       "</g>\n",
       "<!-- predictions&#45;&gt;loss -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>predictions&#45;&gt;loss</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M381.27,-64.75C393.51,-67.91 406.85,-71.37 418.68,-74.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"418.07,-77.89 428.63,-77 419.82,-71.11 418.07,-77.89\"/>\n",
       "</g>\n",
       "<!-- parameters -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>parameters</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"50.05\" cy=\"-20.36\" rx=\"50.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.05\" y=\"-16.66\" font-family=\"Times,serif\" font-size=\"14.00\">parameters</text>\n",
       "</g>\n",
       "<!-- parameters&#45;&gt;model -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>parameters&#45;&gt;model</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M90.61,-31.12C102.13,-34.25 114.85,-37.71 126.88,-40.98\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"126.22,-44.42 136.78,-43.67 128.05,-37.67 126.22,-44.42\"/>\n",
       "</g>\n",
       "<!-- labels -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>labels</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"340.14\" cy=\"-108.36\" rx=\"31.4\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"340.14\" y=\"-104.66\" font-family=\"Times,serif\" font-size=\"14.00\">labels</text>\n",
       "</g>\n",
       "<!-- labels&#45;&gt;loss -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>labels&#45;&gt;loss</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M369.41,-102.05C384.3,-98.73 402.69,-94.63 418.44,-91.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"419.35,-94.49 428.35,-88.9 417.83,-87.66 419.35,-94.49\"/>\n",
       "</g>\n",
       "<!-- loss&#45;&gt;parameters -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>loss&#45;&gt;parameters</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M440.88,-67.53C429.39,-54.1 410.95,-35.74 390.18,-27.36 295.38,10.89 173.21,0.49 104.38,-10.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"103.53,-6.77 94.21,-11.81 104.64,-13.68 103.53,-6.77\"/>\n",
       "<text text-anchor=\"middle\" x=\"253.59\" y=\"-6.16\" font-family=\"Times,serif\" font-size=\"14.00\">update</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7efcac717550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "#caption Detailed training loop\n",
    "#id detailed_loop\n",
    "gv('''ordering=in\n",
    "model[shape=box3d width=1 height=0.7 label=architecture]\n",
    "inputs->model->predictions; parameters->model; labels->loss; predictions->loss\n",
    "loss->parameters[constraint=false label=update]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> jargon: Machine Learning: The training of programs developed by allowing a computer to learn from its experience, rather than through manually coding the individual steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not too hard to imagine what the model might look like for a checkers program. There might be a range of checkers strategies encoded, and some kind of search mechanism, and then the weights could vary how strategies are selected, what parts of the board are focused on during a search, and so forth. But it's not at all obvious what the model might look like for an image recognition program, or for understanding text, or for many other interesting problems we might imagine.\n",
    "\n",
    "What we would like is some kind of function that is so flexible that it could be used to solve any given problem, just by varying its weights. Amazingly enough, this function actually exists! It's the neural network, which we already discussed. That is, if you regard a neural network as a mathematical function, it turns out to be a function which is extremely flexible depending on its weights. A mathematical proof called the *universal approximation theorem* shows that this function can solve any problem to any level of accuracy, in theory. The fact that neural networks are so flexible means that, in practice, they are often a suitable kind of model, and you can focus your effort on the process of training themâ€”that is, of finding good weight assignments.\n",
    "\n",
    "But what about that process?  One could imagine that you might need to find a new \"mechanism\" for automatically updating weights for every problem. This would be laborious. What we'd like here as well is a completely general way to update the weights of a neural network, to make it improve at any given task. Conveniently, this also exists!\n",
    "\n",
    "This is called *stochastic gradient descent* (SGD). We'll see how neural networks and SGD work in detail in <<chapter_mnist_basics>>, as well as explaining the universal approximation theorem. For now, however, we will instead use Samuel's own words: *We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would \"learn\" from its experience.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samuel was working in the 1960s, and since then terminology has changed. Here is the modern deep learning terminology for all the pieces we have discussed:\n",
    "\n",
    "- The functional form of the *model* is called its *architecture* (but be carefulâ€”sometimes people use *model* as a synonym of *architecture*, so this can get confusing).\n",
    "- The *weights* are called *parameters*.\n",
    "- The *predictions* are calculated from the *independent variable*, which is the *data* not including the *labels*.\n",
    "- The *results* of the model are called *predictions*.\n",
    "- The measure of *performance* is called the *loss*.\n",
    "- The loss depends not only on the predictions, but also the correct *labels* (also known as *targets* or the *dependent variable*); e.g., \"dog\" or \"cat.\"\n",
    "\n",
    "After making these changes, our diagram in <<training_loop>> looks like <<detailed_loop>>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations Inherent To Machine Learning\n",
    "\n",
    "From this picture we can now see some fundamental things about training a deep learning model:\n",
    "\n",
    "- A model cannot be created without data.\n",
    "- A model can only learn to operate on the patterns seen in the input data used to train it.\n",
    "- This learning approach only creates *predictions*, not recommended *actions*.\n",
    "- It's not enough to just have examples of input data; we need *labels* for that data too (e.g., pictures of dogs and cats aren't enough to train a model; we need a label for each one, saying which ones are dogs, and which are cats).\n",
    "\n",
    "Generally speaking, we've seen that most organizations that say they don't have enough data, actually mean they don't have enough *labeled* data. If any organization is interested in doing something in practice with a model, then presumably they have some inputs they plan to run their model against. And presumably they've been doing that some other way for a while (e.g., manually, or with some heuristic program), so they have data from those processes! For instance, a radiology practice will almost certainly have an archive of medical scans (since they need to be able to check how their patients are progressing over time), but those scans may not have structured labels containing a list of diagnoses or interventions (since radiologists generally create free-text natural language reports, not structured data). We'll be discussing labeling approaches a lot in this book, because it's such an important issue in practice.\n",
    "\n",
    "Since these kinds of machine learning models can only make *predictions* (i.e., attempt to replicate labels), this can result in a significant gap between organizational goals and model capabilities. For instance, in this book you'll learn how to create a *recommendation system* that can predict what products a user might purchase. This is often used in e-commerce, such as to customize products shown on a home page by showing the highest-ranked items. But such a model is generally created by looking at a user and their buying history (*inputs*) and what they went on to buy or look at (*labels*), which means that the model is likely to tell you about products the user already has or already knows about, rather than new products that they are most likely to be interested in hearing about. That's very different to what, say, an expert at your local bookseller might do, where they ask questions to figure out your taste, and then tell you about authors or series that you've never heard of before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Our Image Recognizer Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see just how our image recognizer code maps to these ideas. We'll put each line into a separate cell, and look at what each one is doing (we won't explain every detail of every parameter yet, but will give a description of the important bits; full details will come later in the book)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line imports all of the fastai.vision library.\n",
    "\n",
    "```python\n",
    "from fastai.vision.all import *\n",
    "```\n",
    "\n",
    "This gives us all of the functions and classes we will need to create a wide variety of computer vision models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> J: A lot of Python coders recommend avoiding importing a whole library like this (using the `import *` syntax), because in large software projects it can cause problems. However, for interactive work such as in a Jupyter notebook, it works great. The fastai library is specially designed to support this kind of interactive use, and it will only import the necessary pieces into your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second line downloads a standard dataset from the [fast.ai datasets collection](https://course.fast.ai/datasets) (if not previously downloaded) to your server, extracts it (if not previously extracted), and returns a `Path` object with the extracted location:\n",
    "\n",
    "```python\n",
    "path = untar_data(URLs.PETS)/'images'\n",
    "```\n",
    "\n",
    "> S: Throughout my time studying at fast.ai, and even still today, I've learned a lot about productive coding practices. The fastai library and fast.ai notebooks are full of great little tips that have helped make me a better programmer. For instance, notice that the fastai library doesn't just return a string containing the path to the dataset, but a `Path` object. This is a really useful class from the Python 3 standard library that makes accessing files and directories much easier. If you haven't come across it before, be sure to check out its documentation or a tutorial and try it out. Note that the https://book.fast.ai[website] contains links to recommended tutorials for each chapter. I'll keep letting you know about little coding tips I've found useful as we come across them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the third line we define a function, `is_cat`, which labels cats based on a filename rule provided by the dataset creators:\n",
    "```python\n",
    "def is_cat(x): return x[0].isupper()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use that function in the fourth line, which tells fastai what kind of dataset we have and how it is structured:\n",
    "\n",
    "```python\n",
    "dls = ImageDataLoaders.from_name_func(\n",
    "    path, get_image_files(path), valid_pct=0.2, seed=42,\n",
    "    label_func=is_cat, item_tfms=Resize(224))\n",
    "```\n",
    "\n",
    "There are various different classes for different kinds of deep learning datasets and problemsâ€”here we're using `ImageDataLoaders`. The first part of the class name will generally be the type of data you have, such as image, or text.\n",
    "\n",
    "The other important piece of information that we have to tell fastai is how to get the labels from the dataset. Computer vision datasets are normally structured in such a way that the label for an image is part of the filename, or pathâ€”most commonly the parent folder name. fastai comes with a number of standardized labeling methods, and ways to write your own. Here we're telling fastai to use the `is_cat` function we just defined.\n",
    "\n",
    "Finally, we define the `Transform`s that we need. A `Transform` contains code that is applied automatically during training; fastai includes many predefined `Transform`s, and adding new ones is as simple as creating a Python function. There are two kinds: `item_tfms` are applied to each item (in this case, each item is resized to a 224-pixel square), while `batch_tfms` are applied to a *batch* of items at a time using the GPU, so they're particularly fast (we'll see many examples of these throughout this book).\n",
    "\n",
    "Why 224 pixels? This is the standard size for historical reasons (old pretrained models require this size exactly), but you can pass pretty much anything. If you increase the size, you'll often get a model with better results (since it will be able to focus on more details), but at the price of speed and memory consumption; the opposite is true if you decrease the size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Classification and Regression: _classification_ and _regression_ have very specific meanings in machine learning. These are the two main types of model that we will be investigating in this book. A classification model is one which attempts to predict a class, or category. That is, it's predicting from a number of discrete possibilities, such as \"dog\" or \"cat.\" A regression model is one which attempts to predict one or more numeric quantities, such as a temperature or a location. Sometimes people use the word _regression_ to refer to a particular kind of model called a _linear regression model_; this is a bad practice, and we won't be using that terminology in this book!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pet dataset contains 7,390 pictures of dogs and cats, consisting of 37 different breeds. Each image is labeled using its filename: for instance the file *great\\_pyrenees\\_173.jpg* is the 173rd example of an image of a Great Pyrenees breed dog in the dataset. The filenames start with an uppercase letter if the image is a cat, and a lowercase letter otherwise. We have to tell fastai how to get labels from the filenames, which we do by calling `from_name_func` (which means that labels can be extracted using a function applied to the filename), and passing `is_cat`, which returns `x[0].isupper()`, which evaluates to `True` if the first letter is uppercase (i.e., it's a cat).\n",
    "\n",
    "The most important parameter to mention here is `valid_pct=0.2`. This tells fastai to hold out 20% of the data and *not use it for training the model at all*. This 20% of the data is called the *validation set*; the remaining 80% is called the *training set*. The validation set is used to measure the accuracy of the model. By default, the 20% that is held out is selected randomly. The parameter `seed=42` sets the *random seed* to the same value every time we run this code, which means we get the same validation set every time we run itâ€”this way, if we change our model and retrain it, we know that any differences are due to the changes to the model, not due to having a different random validation set.\n",
    "\n",
    "fastai will *always* show you your model's accuracy using *only* the validation set, *never* the training set. This is absolutely critical, because if you train a large enough model for a long enough time, it will eventually memorize the label of every item in your dataset! The result will not actually be a useful model, because what we care about is how well our model works on *previously unseen images*. That is always our goal when creating a model: for it to be useful on data that the model only sees in the future, after it has been trained.\n",
    "\n",
    "Even when your model has not fully memorized all your data, earlier on in training it may have memorized certain parts of it. As a result, the longer you train for, the better your accuracy will get on the training set; the validation set accuracy will also improve for a while, but eventually it will start getting worse as the model starts to memorize the training set, rather than finding generalizable underlying patterns in the data. When this happens, we say that the model is *overfitting*.\n",
    "\n",
    "<<img_overfit>> shows what happens when you overfit, using a simplified example where we have just one parameter, and some randomly generated data based on the function `x**2`. As you can see, although the predictions in the overfit model are accurate for data near the observed data points, they are way off when outside of that range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/fastai/fastbook/blob/master/images/att_00000.png?raw=1\" alt=\"Example of overfitting\" caption=\"Example of overfitting\" id=\"img_overfit\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overfitting is the single most important and challenging issue** when training for all machine learning practitioners, and all algorithms. As you will see, it is very easy to create a model that does a great job at making predictions on the exact data it has been trained on, but it is much harder to make accurate predictions on data the model has never seen before. And of course, this is the data that will actually matter in practice. For instance, if you create a handwritten digit classifier (as we will very soon!) and use it to recognize numbers written on checks, then you are never going to see any of the numbers that the model was trained onâ€”checks will have slightly different variations of writing to deal with. You will learn many methods to avoid overfitting in this book. However, you should only use those methods after you have confirmed that overfitting is actually occurring (i.e., you have actually observed the validation accuracy getting worse during training). We often see practitioners using over-fitting avoidance techniques even when they have enough data that they didn't need to do so, ending up with a model that may be less accurate than what they could have achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> important: Validation Set: When you train a model, you must _always_ have both a training set and a validation set, and must measure the accuracy of your model only on the validation set. If you train for too long, with not enough data, you will see the accuracy of your model start to get worse; this is called _overfitting_. fastai defaults `valid_pct` to `0.2`, so even if you forget, fastai will create a validation set for you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fifth line of the code training our image recognizer tells fastai to create a *convolutional neural network* (CNN) and specifies what *architecture* to use (i.e. what kind of model to create), what data we want to train it on, and what *metric* to use:\n",
    "\n",
    "```python\n",
    "learn = vision_learner(dls, resnet34, metrics=error_rate)\n",
    "```\n",
    "\n",
    "Why a CNN? It's the current state-of-the-art approach to creating computer vision models. We'll be learning all about how CNNs work in this book. Their structure is inspired by how the human vision system works.\n",
    "\n",
    "There are many different architectures in fastai, which we will introduce in this book (as well as discussing how to create your own). Most of the time, however, picking an architecture isn't a very important part of the deep learning process. It's something that academics love to talk about, but in practice it is unlikely to be something you need to spend much time on. There are some standard architectures that work most of the time, and in this case we're using one called _ResNet_ that we'll be talking a lot about during the book; it is both fast and accurate for many datasets and problems. The `34` in `resnet34` refers to the number of layers in this variant of the architecture (other options are `18`, `50`, `101`, and `152`). Models using architectures with more layers take longer to train, and are more prone to overfitting (i.e. you can't train them for as many epochs before the accuracy on the validation set starts getting worse). On the other hand, when using more data, they can be quite a bit more accurate.\n",
    "\n",
    "What is a metric? A *metric* is a function that measures the quality of the model's predictions using the validation set, and will be printed at the end of each *epoch*. In this case, we're using `error_rate`, which is a function provided by fastai that does just what it says: tells you what percentage of images in the validation set are being classified incorrectly. Another common metric for classification is `accuracy` (which is just `1.0 - error_rate`). fastai provides many more, which will be discussed throughout this book.\n",
    "\n",
    "The concept of a metric may remind you of *loss*, but there is an important distinction. The entire purpose of loss is to define a \"measure of performance\" that the training system can use to update weights automatically. In other words, a good choice for loss is a choice that is easy for stochastic gradient descent to use. But a metric is defined for human consumption, so a good metric is one that is easy for you to understand, and that hews as closely as possible to what you want the model to do. At times, you might decide that the loss function is a suitable metric, but that is not necessarily the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`vision_learner` also has a parameter `pretrained`, which defaults to `True` (so it's used in this case, even though we haven't specified it), which sets the weights in your model to values that have already been trained by experts to recognize a thousand different categories across 1.3 million photos (using the famous [*ImageNet* dataset](http://www.image-net.org/)). A model that has weights that have already been trained on some other dataset is called a *pretrained model*. You should nearly always use a pretrained model, because it means that your model, before you've even shown it any of your data, is already very capable. And, as you'll see, in a deep learning model many of these capabilities are things you'll need, almost regardless of the details of your project. For instance, parts of pretrained models will handle edge, gradient, and color detection, which are needed for many tasks.\n",
    "\n",
    "When using a pretrained model, `vision_learner` will remove the last layer, since that is always specifically customized to the original training task (i.e. ImageNet dataset classification), and replace it with one or more new layers with randomized weights, of an appropriate size for the dataset you are working with. This last part of the model is known as the *head*.\n",
    "\n",
    "Using pretrained models is the *most* important method we have to allow us to train more accurate models, more quickly, with less data, and less time and money. You might think that would mean that using pretrained models would be the most studied area in academic deep learning... but you'd be very, very wrong! The importance of pretrained models is generally not recognized or discussed in most courses, books, or software library features, and is rarely considered in academic papers. As we write this at the start of 2020, things are just starting to change, but it's likely to take a while. So be careful: most people you speak to will probably greatly underestimate what you can do in deep learning with few resources, because they probably won't deeply understand how to use pretrained models.\n",
    "\n",
    "Using a pretrained model for a task different to what it was originally trained for is known as *transfer learning*. Unfortunately, because transfer learning is so under-studied, few domains have pretrained models available. For instance, there are currently few pretrained models available in medicine, making transfer learning challenging to use in that domain. In addition, it is not yet well understood how to use transfer learning for tasks such as time series analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> jargon: Transfer learning: Using a pretrained model for a task different to what it was originally trained for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sixth line of our code tells fastai how to *fit* the model:\n",
    "\n",
    "```python\n",
    "learn.fine_tune(1)\n",
    "```\n",
    "\n",
    "As we've discussed, the architecture only describes a *template* for a mathematical function; it doesn't actually do anything until we provide values for the millions of parameters it contains.\n",
    "\n",
    "This is the key to deep learningâ€”determining how to fit the parameters of a model to get it to solve your problem. In order to fit a model, we have to provide at least one piece of information: how many times to look at each image (known as number of *epochs*). The number of epochs you select will largely depend on how much time you have available, and how long you find it takes in practice to fit your model. If you select a number that is too small, you can always train for more epochs later.\n",
    "\n",
    "But why is the method called `fine_tune`, and not `fit`? fastai actually *does* have a method called `fit`, which does indeed fit a model (i.e. look at images in the training set multiple times, each time updating the parameters to make the predictions closer and closer to the target labels). But in this case, we've started with a pretrained model, and we don't want to throw away all those capabilities that it already has. As you'll learn in this book, there are some important tricks to adapt a pretrained model for a new datasetâ€”a process called *fine-tuning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> jargon: Fine-tuning: A transfer learning technique where the parameters of a pretrained model are updated by training for additional epochs using a different task to that used for pretraining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use the `fine_tune` method, fastai will use these tricks for you. There are a few parameters you can set (which we'll discuss later), but in the default form shown here, it does two steps:\n",
    "\n",
    "1. Use one epoch to fit just those parts of the model necessary to get the new random head to work correctly with your dataset.\n",
    "1. Use the number of epochs requested when calling the method to fit the entire model, updating the weights of the later layers (especially the head) faster than the earlier layers (which, as we'll see, generally don't require many changes from the pretrained weights).\n",
    "\n",
    "The *head* of a model is the part that is newly added to be specific to the new dataset. An *epoch* is one complete pass through the dataset. After calling `fit`, the results after each epoch are printed, showing the epoch number, the training and validation set losses (the \"measure of performance\" used for training the model), and any *metrics* you've requested (error rate, in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we have an image recognizer that is working very well, but we have no idea what it is actually doing! Although many people complain that deep learning results in impenetrable \"black box\" models (that is, something that gives predictions but that no one can understand), this really couldn't be further from the truth. There is a vast body of research showing how to deeply inspect deep learning models, and get rich insights from them. Having said that, all kinds of machine learning models (including deep learning, and traditional statistical models) can be challenging to fully understand, especially when considering how they will behave when coming across data that is very different to the data used to train them. We'll be discussing this issue throughout this book.\n",
    "\n",
    "In 2013 a PhD student, Matt Zeiler, and his supervisor, Rob Fergus, published the paper [\"Visualizing and Understanding Convolutional Networks\"](https://arxiv.org/pdf/1311.2901.pdf), which showed how to visualize the neural network weights learned in each layer of a model. They carefully analyzed the model that won the 2012 ImageNet competition, and used this analysis to greatly improve the model, such that they were able to go on to win the 2013 competition! <<img_layer1>> is the picture that they published of the first layer's weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/fastai/fastbook/blob/master/images/layer1.png?raw=1\" alt=\"Activations of the first layer of a CNN\" width=\"300\" caption=\"Activations of the first layer of a CNN (courtesy of Matthew D. Zeiler and Rob Fergus)\" id=\"img_layer1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This picture requires some explanation. For each layer, the image part with the light gray background shows the reconstructed weights pictures, and the larger section at the bottom shows the parts of the training images that most strongly matched each set of weights. For layer 1, what we can see is that the model has discovered weights that represent diagonal, horizontal, and vertical edges, as well as various different gradients. (Note that for each layer only a subset of the features are shown; in practice there are thousands across all of the layers.) These are the basic building blocks that the model has learned for computer vision. They have been widely analyzed by neuroscientists and computer vision researchers, and it turns out that these learned building blocks are very similar to the basic visual machinery in the human eye, as well as the handcrafted computer vision features that were developed prior to the days of deep learning. The next layer is represented in <<img_layer2>>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/fastai/fastbook/blob/master/images/layer2.png?raw=1\" alt=\"Activations of the second layer of a CNN\" width=\"800\" caption=\"Activations of the second layer of a CNN (courtesy of Matthew D. Zeiler and Rob Fergus)\" id=\"img_layer2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For layer 2, there are nine examples of weight reconstructions for each of the features found by the model. We can see that the model has learned to create feature detectors that look for corners, repeating lines, circles, and other simple patterns. These are built from the basic building blocks developed in the first layer. For each of these, the right-hand side of the picture shows small patches from actual images which these features most closely match. For instance, the particular pattern in row 2, column 1 matches the gradients and textures associated with sunsets.\n",
    "\n",
    "<<img_layer3>> shows the image from the paper showing the results of reconstructing the features of layer 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/fastai/fastbook/blob/master/images/chapter2_layer3.PNG?raw=1\" alt=\"Activations of the third layer of a CNN\" width=\"800\" caption=\"Activations of the third layer of a CNN (courtesy of Matthew D. Zeiler and Rob Fergus)\" id=\"img_layer3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see by looking at the righthand side of this picture, the features are now able to identify and match with higher-level semantic components, such as car wheels, text, and flower petals. Using these components, layers four and five can identify even higher-level concepts, as shown in <<img_layer4>>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/fastai/fastbook/blob/master/images/chapter2_layer4and5.PNG?raw=1\" alt=\"Activations of layers 4 and 5 of a CNN\" width=\"800\" caption=\"Activations of layers 4 and 5 of a CNN (courtesy of Matthew D. Zeiler and Rob Fergus)\" id=\"img_layer4\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This article was studying an older model called *AlexNet* that only contained five layers. Networks developed since then can have hundreds of layersâ€”so you can imagine how rich the features developed by these models can be!\n",
    "\n",
    "When we fine-tuned our pretrained model earlier, we adapted what those last layers focus on (flowers, humans, animals) to specialize on the cats versus dogs problem. More generally, we could specialize such a pretrained model on many different tasks. Let's have a look at some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Recognizers Can Tackle Non-Image Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An image recognizer can, as its name suggests, only recognize images. But a lot of things can be represented as images, which means that an image recogniser can learn to complete many tasks.\n",
    "\n",
    "For instance, a sound can be converted to a spectrogram, which is a chart that shows the amount of each frequency at each time in an audio file. Fast.ai student Ethan Sutin used this approach to easily beat the published accuracy of a state-of-the-art [environmental sound detection model](https://medium.com/@etown/great-results-on-audio-classification-with-fastai-library-ccaf906c5f52) using a dataset of 8,732 urban sounds. fastai's `show_batch` clearly shows how each different sound has a quite distinctive spectrogram, as you can see in <<img_spect>>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"show_batch with spectrograms of sounds\" width=\"400\" caption=\"show_batch with spectrograms of sounds\" id=\"img_spect\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00012.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A time series can easily be converted into an image by simply plotting the time series on a graph. However, it is often a good idea to try to represent your data in a way that makes it as easy as possible to pull out the most important components. In a time series, things like seasonality and anomalies are most likely to be of interest. There are various transformations available for time series data. For instance, fast.ai student Ignacio Oguiza created images from a time series dataset for olive oil classification, using a technique called Gramian Angular Difference Field (GADF); you can see the result in <<ts_image>>. He then fed those images to an image classification model just like the one you see in this chapter. His results, despite having only 30 training set images, were well over 90% accurate, and close to the state of the art."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Converting a time series into an image\" width=\"700\" caption=\"Converting a time series into an image\" id=\"ts_image\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00013.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting fast.ai student project example comes from Gleb Esman. He was working on fraud detection at Splunk, using a dataset of users' mouse movements and mouse clicks. He turned these into pictures by drawing an image where the position, speed, and acceleration of the mouse pointer was displayed using coloured lines, and the clicks were displayed using [small colored circles](https://www.splunk.com/en_us/blog/security/deep-learning-with-splunk-and-tensorflow-for-security-catching-the-fraudster-in-neural-networks-with-behavioral-biometrics.html), as shown in <<splunk>>. He then fed this into an image recognition model just like the one we've used in this chapter, and it worked so well that it led to a patent for this approach to fraud analytics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Converting computer mouse behavior to an image\" width=\"450\" caption=\"Converting computer mouse behavior to an image\" id=\"splunk\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00014.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just covered a lot of information so let's recap briefly, <<dljargon>> provides a handy vocabulary.\n",
    "\n",
    "```asciidoc\n",
    "[[dljargon]]\n",
    ".Deep learning vocabulary\n",
    "[options=\"header\"]\n",
    "|=====\n",
    "| Term | Meaning\n",
    "|Label | The data that we're trying to predict, such as \"dog\" or \"cat\"\n",
    "|Architecture | The _template_ of the model that we're trying to fit; the actual mathematical function that we're passing the input data and parameters to\n",
    "|Model | The combination of the architecture with a particular set of parameters\n",
    "|Parameters | The values in the model that change what task it can do, and are updated through model training\n",
    "|Fit | Update the parameters of the model such that the predictions of the model using the input data match the target labels\n",
    "|Train | A synonym for _fit_\n",
    "|Pretrained model | A model that has already been trained, generally using a large dataset, and will be fine-tuned\n",
    "|Fine-tune | Update a pretrained model for a different task\n",
    "|Epoch | One complete pass through the input data\n",
    "|Loss | A measure of how good the model is, chosen to drive training via SGD\n",
    "|Metric | A measurement of how good the model is, using the validation set, chosen for human consumption\n",
    "|Validation set | A set of data held out from training, used only for measuring how good the model is\n",
    "|Training set | The data used for fitting the model; does not include any data from the validation set\n",
    "|Overfitting | Training a model in such a way that it _remembers_ specific features of the input data, rather than generalizing well to data not seen during training\n",
    "|CNN | Convolutional neural network; a type of neural network that works particularly well for computer vision tasks\n",
    "|=====\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ever have any questions about a fastai method, you should use the function `doc`, passing it the method name:\n",
    "\n",
    "```python\n",
    "doc(learn.predict)\n",
    "```\n",
    "\n",
    "This will make a small window pop up with content like this:\n",
    "\n",
    "<img src=\"https://github.com/fastai/fastbook/blob/master/images/doc_ex.png?raw=1\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> jargon: Tabular: Data that is in the form of a table, such as from a spreadsheet, database, or CSV file. A tabular model is a model that tries to predict one column of a table based on information in other columns of the table."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
